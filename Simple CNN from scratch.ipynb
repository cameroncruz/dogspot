{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Breed Identification\n",
    "Cameron Cruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from tqdm import trange\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logger(log_path):\n",
    "    \"\"\"Sets the logger to log info in terminal and file `log_path`.\n",
    "    In general, it is useful to have a logger so that every output to the terminal is saved\n",
    "    in a permanent file. Here we save it to `model_dir/train.log`.\n",
    "    Example:\n",
    "    ```\n",
    "    logging.info(\"Starting training...\")\n",
    "    ```\n",
    "    Args:\n",
    "        log_path: (string) where to log\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        # Logging to a file\n",
    "        file_handler = logging.FileHandler(log_path)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict of floats in json file\n",
    "    Args:\n",
    "        d: (dict) of float-castable values (np.float, int, float, etc.)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )\n",
    "        d = {k: float(v) for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training set image ids and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       10222\n",
       "breed    10222\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/labels.csv')\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit One-hot Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "with open('classes.txt', 'r') as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "lb.fit_transform(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train-eval split (90-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = data['id'].tolist()\n",
    "labels = data['breed'].tolist()\n",
    "\n",
    "one_hot_labels = lb.transform(labels)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(file_ids, one_hot_labels,\n",
    "                                                   test_size=0.05,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set params and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "num_classes = len(classes)\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "num_epochs = 25\n",
    "train_size = len(X_train)\n",
    "eval_size = len(X_val)\n",
    "model_dir = 'model'\n",
    "set_logger(os.path.join(model_dir, 'train.log'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parse_fn to get image from a file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fn(file_id, label):\n",
    "    image_string = tf.read_file('data' + os.sep + 'train' + os.sep\n",
    "                                + file_id + '.jpg')\n",
    "    img = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize_images(img, [image_size, image_size],\n",
    "                                method=tf.image.ResizeMethod.AREA)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for real-time data augmentation during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(img, label):\n",
    "    img = tf.image.random_flip_left_right(img) # Random horizontal flip\n",
    "    img = tf.image.random_brightness(img, max_delta=32.0 / 255.0) # Random brightness\n",
    "    img = tf.image.random_saturation(img, lower=0.5, upper=1.5) # Random saturation\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0) # Ensure values are still within [0, 1]\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input_fn to create data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(mode, X, Y):\n",
    "    assert len(X) == len(Y)\n",
    "    \n",
    "    num_samples = len(X)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        dataset = (tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
    "                                                       tf.constant(Y)))\n",
    "                   .shuffle(num_samples)\n",
    "                   .map(parse_fn, num_parallel_calls=4)\n",
    "                   .map(train_fn, num_parallel_calls=4)\n",
    "                   .batch(batch_size)\n",
    "                   .prefetch(1)\n",
    "                  )\n",
    "    else:\n",
    "        dataset = (tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
    "                                                      tf.constant(Y)))\n",
    "                  .map(parse_fn)\n",
    "                  .batch(batch_size)\n",
    "                  .prefetch(1)\n",
    "                  )\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    imgs, labls = iterator.get_next()\n",
    "    iterator_init_op = iterator.initializer\n",
    "    \n",
    "    inputs = {'images': imgs, 'labels': labls,\n",
    "              'iterator_init_op': iterator_init_op}\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(is_training, inputs):\n",
    "    images = inputs['images']\n",
    "\n",
    "    #assert images.get_shape().as_list() == [None, image_size, image_size, 3]\n",
    "\n",
    "    out = images\n",
    "    \n",
    "    num_channels = 16\n",
    "    bn_momentum = 0.90\n",
    "    channels = [num_channels, num_channels * 2, num_channels * 4, num_channels * 8]\n",
    "    for i, c in enumerate(channels):\n",
    "        with tf.variable_scope('block_{}'.format(i+1)):\n",
    "            out = tf.layers.conv2d(out, c, 3, padding='same')\n",
    "            out = tf.layers.batch_normalization(out, momentum=bn_momentum, training=is_training)\n",
    "            out = tf.nn.relu(out)\n",
    "            out = tf.layers.max_pooling2d(out, 2, 2)\n",
    "\n",
    "    assert out.get_shape().as_list() == [None, 4, 4, num_channels * 8]\n",
    "\n",
    "    out = tf.reshape(out, [-1, 4 * 4 * num_channels * 8])\n",
    "    with tf.variable_scope('fc_1'):\n",
    "        out = tf.layers.dense(out, num_channels * 8)\n",
    "        out = tf.layers.batch_normalization(out, momentum=bn_momentum, training=is_training)\n",
    "        #out = tf.layers.dropout(out, rate=0.2, training=is_training)\n",
    "        out = tf.nn.relu(out)\n",
    "    with tf.variable_scope('fc_2'):\n",
    "        logits = tf.layers.dense(out, num_classes)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define graph ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(mode, inputs, reuse=False):\n",
    "    is_training = (mode == 'train')\n",
    "    labels = inputs['labels']\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # MODEL: define the layers of the model\n",
    "    with tf.variable_scope('model', reuse=reuse):\n",
    "        # Compute the output distribution of the model and the predictions\n",
    "        logits = build_model(is_training, inputs)\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "\n",
    "    # Define loss and accuracy\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(labels, 1), predictions), tf.float32))\n",
    "\n",
    "    # Define training step that minimizes the loss with the Adam optimizer\n",
    "    if is_training:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        # Add a dependency to update the moving mean and variance for batch normalization\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # METRICS AND SUMMARIES\n",
    "    # Metrics for evaluation using tf.metrics (average over whole dataset)\n",
    "    with tf.variable_scope(\"metrics\"):\n",
    "        metrics = {\n",
    "            'accuracy': tf.metrics.accuracy(labels=tf.argmax(labels, 1), predictions=tf.argmax(logits, 1)),\n",
    "            'loss': tf.metrics.mean(loss)\n",
    "        }\n",
    "\n",
    "    # Group the update ops for the tf.metrics\n",
    "    update_metrics_op = tf.group(*[op for _, op in metrics.values()])\n",
    "\n",
    "    # Get the op to reset the local variables used in tf.metrics\n",
    "    metric_variables = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metrics\")\n",
    "    metrics_init_op = tf.variables_initializer(metric_variables)\n",
    "\n",
    "    # Summaries for training\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.image('train_image', inputs['images'])\n",
    "\n",
    "    #TODO: if mode == 'eval': ?\n",
    "    # Add incorrectly labeled images\n",
    "    mask = tf.not_equal(labels, predictions)\n",
    "\n",
    "    # Add a different summary to know how they were misclassified\n",
    "    \"\"\"\n",
    "    for label in range(0, num_classes):\n",
    "        mask_label = tf.logical_and(mask, tf.equal(predictions, label))\n",
    "        incorrect_image_label = tf.boolean_mask(inputs['images'], mask_label)\n",
    "        tf.summary.image('incorrectly_labeled_{}'.format(label), incorrect_image_label)\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # MODEL SPECIFICATION\n",
    "    # Create the model specification and return it\n",
    "    # It contains nodes or operations in the graph that will be used for training and evaluation\n",
    "    model_spec = inputs\n",
    "    model_spec['variable_init_op'] = tf.global_variables_initializer()\n",
    "    model_spec[\"predictions\"] = predictions\n",
    "    model_spec['loss'] = loss\n",
    "    model_spec['accuracy'] = accuracy\n",
    "    model_spec['metrics_init_op'] = metrics_init_op\n",
    "    model_spec['metrics'] = metrics\n",
    "    model_spec['update_metrics'] = update_metrics_op\n",
    "    model_spec['summary_op'] = tf.summary.merge_all()\n",
    "\n",
    "    if is_training:\n",
    "        model_spec['train_op'] = train_op\n",
    "\n",
    "    return model_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sess(sess, model_spec, num_steps, writer):\n",
    "    # Get relevant graph operations or nodes needed for training\n",
    "    loss = model_spec['loss']\n",
    "    train_op = model_spec['train_op']\n",
    "    update_metrics = model_spec['update_metrics']\n",
    "    metrics = model_spec['metrics']\n",
    "    summary_op = model_spec['summary_op']\n",
    "    global_step = tf.train.get_global_step()\n",
    "\n",
    "    # Load the training dataset into the pipeline and initialize the metrics local variables\n",
    "    sess.run(model_spec['iterator_init_op'])\n",
    "    sess.run(model_spec['metrics_init_op'])\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # Evaluate summaries for tensorboard only once in a while\n",
    "        if i % 1 == 0:\n",
    "            # Perform a mini-batch update\n",
    "            _, _, loss_val, summ, global_step_val = sess.run([train_op, update_metrics, loss,\n",
    "                                                              summary_op, global_step])\n",
    "            # Write summaries for tensorboard\n",
    "            writer.add_summary(summ, global_step_val)\n",
    "        else:\n",
    "            _, _, loss_val = sess.run([train_op, update_metrics, loss])\n",
    "        # Log the loss in the tqdm progress bar\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_val))\n",
    "\n",
    "\n",
    "    metrics_values = {k: v[0] for k, v in metrics.items()}\n",
    "    metrics_val = sess.run(metrics_values)\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_val.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sess(sess, model_spec, num_steps, writer=None):\n",
    "    update_metrics = model_spec['update_metrics']\n",
    "    eval_metrics = model_spec['metrics']\n",
    "    global_step = tf.train.get_global_step()\n",
    "\n",
    "    # Load the evaluation dataset into the pipeline and initialize the metrics init op\n",
    "    sess.run(model_spec['iterator_init_op'])\n",
    "    sess.run(model_spec['metrics_init_op'])\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        sess.run(update_metrics)\n",
    "\n",
    "    # Get the values of the metrics\n",
    "    metrics_values = {k: v[0] for k, v in eval_metrics.items()}\n",
    "    metrics_val = sess.run(metrics_values)\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_val.items())\n",
    "    logging.info(\"- Eval metrics: \" + metrics_string)\n",
    "\n",
    "    # Add summaries manually to writer at global_step_val\n",
    "    if writer is not None:\n",
    "        global_step_val = sess.run(global_step)\n",
    "        for tag, val in metrics_val.items():\n",
    "            summ = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=val)])\n",
    "            writer.add_summary(summ, global_step_val)\n",
    "\n",
    "    return metrics_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = input_fn('train', X_train, y_train)\n",
    "eval_inputs = input_fn('eval', X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating the model...\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Creating the model...\")\n",
    "train_model_spec = model_fn('train', train_inputs, reuse=tf.AUTO_REUSE)\n",
    "eval_model_spec = model_fn('eval', eval_inputs, reuse=tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting training for 25 epoch(s)\n",
      "Epoch 1/25\n",
      "100%|██████████| 304/304 [00:18<00:00, 16.42it/s, loss=4.659]\n",
      "- Train metrics: accuracy: 0.017 ; loss: 4.880\n",
      "- Eval metrics: accuracy: 0.029 ; loss: 4.714\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-1\n",
      "Epoch 2/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.56it/s, loss=4.412]\n",
      "- Train metrics: accuracy: 0.040 ; loss: 4.587\n",
      "- Eval metrics: accuracy: 0.033 ; loss: 4.576\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-2\n",
      "Epoch 3/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.45it/s, loss=4.440]\n",
      "- Train metrics: accuracy: 0.058 ; loss: 4.428\n",
      "- Eval metrics: accuracy: 0.047 ; loss: 4.485\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-3\n",
      "Epoch 4/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.56it/s, loss=4.169]\n",
      "- Train metrics: accuracy: 0.072 ; loss: 4.311\n",
      "- Eval metrics: accuracy: 0.064 ; loss: 4.416\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-4\n",
      "Epoch 5/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.58it/s, loss=4.496]\n",
      "- Train metrics: accuracy: 0.095 ; loss: 4.204\n",
      "- Eval metrics: accuracy: 0.066 ; loss: 4.361\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-5\n",
      "Epoch 6/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.50it/s, loss=4.146]\n",
      "- Train metrics: accuracy: 0.108 ; loss: 4.110\n",
      "- Eval metrics: accuracy: 0.072 ; loss: 4.309\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-6\n",
      "Epoch 7/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.53it/s, loss=3.709]\n",
      "- Train metrics: accuracy: 0.123 ; loss: 4.024\n",
      "- Eval metrics: accuracy: 0.072 ; loss: 4.275\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-7\n",
      "Epoch 8/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.48it/s, loss=3.918]\n",
      "- Train metrics: accuracy: 0.140 ; loss: 3.944\n",
      "- Eval metrics: accuracy: 0.084 ; loss: 4.252\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-8\n",
      "Epoch 9/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.50it/s, loss=3.801]\n",
      "- Train metrics: accuracy: 0.159 ; loss: 3.852\n",
      "- Eval metrics: accuracy: 0.062 ; loss: 4.221\n",
      "Epoch 10/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.43it/s, loss=3.848]\n",
      "- Train metrics: accuracy: 0.174 ; loss: 3.772\n",
      "- Eval metrics: accuracy: 0.078 ; loss: 4.179\n",
      "Epoch 11/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.43it/s, loss=3.870]\n",
      "- Train metrics: accuracy: 0.192 ; loss: 3.689\n",
      "- Eval metrics: accuracy: 0.082 ; loss: 4.176\n",
      "Epoch 12/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.41it/s, loss=3.535]\n",
      "- Train metrics: accuracy: 0.208 ; loss: 3.619\n",
      "- Eval metrics: accuracy: 0.080 ; loss: 4.165\n",
      "Epoch 13/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.25it/s, loss=3.045]\n",
      "- Train metrics: accuracy: 0.226 ; loss: 3.539\n",
      "- Eval metrics: accuracy: 0.078 ; loss: 4.152\n",
      "Epoch 14/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.20it/s, loss=2.858]\n",
      "- Train metrics: accuracy: 0.242 ; loss: 3.463\n",
      "- Eval metrics: accuracy: 0.094 ; loss: 4.171\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-14\n",
      "Epoch 15/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.08it/s, loss=3.468]\n",
      "- Train metrics: accuracy: 0.268 ; loss: 3.373\n",
      "- Eval metrics: accuracy: 0.084 ; loss: 4.110\n",
      "Epoch 16/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.29it/s, loss=3.062]\n",
      "- Train metrics: accuracy: 0.279 ; loss: 3.306\n",
      "- Eval metrics: accuracy: 0.096 ; loss: 4.107\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-16\n",
      "Epoch 17/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.23it/s, loss=2.815]\n",
      "- Train metrics: accuracy: 0.306 ; loss: 3.221\n",
      "- Eval metrics: accuracy: 0.092 ; loss: 4.101\n",
      "Epoch 18/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.35it/s, loss=3.562]\n",
      "- Train metrics: accuracy: 0.323 ; loss: 3.131\n",
      "- Eval metrics: accuracy: 0.096 ; loss: 4.091\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-18\n",
      "Epoch 19/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.36it/s, loss=3.314]\n",
      "- Train metrics: accuracy: 0.347 ; loss: 3.047\n",
      "- Eval metrics: accuracy: 0.094 ; loss: 4.098\n",
      "Epoch 20/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.39it/s, loss=3.133]\n",
      "- Train metrics: accuracy: 0.368 ; loss: 2.975\n",
      "- Eval metrics: accuracy: 0.094 ; loss: 4.088\n",
      "Epoch 21/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.43it/s, loss=3.171]\n",
      "- Train metrics: accuracy: 0.382 ; loss: 2.891\n",
      "- Eval metrics: accuracy: 0.082 ; loss: 4.127\n",
      "Epoch 22/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.47it/s, loss=3.000]\n",
      "- Train metrics: accuracy: 0.404 ; loss: 2.801\n",
      "- Eval metrics: accuracy: 0.086 ; loss: 4.125\n",
      "Epoch 23/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.37it/s, loss=2.621]\n",
      "- Train metrics: accuracy: 0.423 ; loss: 2.726\n",
      "- Eval metrics: accuracy: 0.082 ; loss: 4.105\n",
      "Epoch 24/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.39it/s, loss=3.063]\n",
      "- Train metrics: accuracy: 0.434 ; loss: 2.654\n",
      "- Eval metrics: accuracy: 0.104 ; loss: 4.085\n",
      "- Found new best accuracy, saving in model/best_weights/after-epoch-24\n",
      "Epoch 25/25\n",
      "100%|██████████| 304/304 [00:17<00:00, 17.40it/s, loss=3.226]\n",
      "- Train metrics: accuracy: 0.462 ; loss: 2.568\n",
      "- Eval metrics: accuracy: 0.094 ; loss: 4.134\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting training for {} epoch(s)\".format(num_epochs))\n",
    "last_saver = tf.train.Saver() # will keep last 5 epochs\n",
    "best_saver = tf.train.Saver(max_to_keep=1)  # only keep 1 best checkpoint (best on eval)\n",
    "begin_at_epoch = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize model variables\n",
    "    sess.run(train_model_spec['variable_init_op'])\n",
    "\n",
    "    # Reload weights from directory if specified\n",
    "    \"\"\"\n",
    "    if restore_from is not None:\n",
    "        logging.info(\"Restoring parameters from {}\".format(restore_from))\n",
    "        if os.path.isdir(restore_from):\n",
    "            restore_from = tf.train.latest_checkpoint(restore_from)\n",
    "            begin_at_epoch = int(restore_from.split('-')[-1])\n",
    "        last_saver.restore(sess, restore_from)\n",
    "    \"\"\"\n",
    "\n",
    "    # For tensorboard (takes care of writing summaries to files)\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(model_dir, 'train_summaries'), sess.graph)\n",
    "    eval_writer = tf.summary.FileWriter(os.path.join(model_dir, 'eval_summaries'), sess.graph)\n",
    "\n",
    "    best_eval_acc = 0.0\n",
    "    for epoch in range(begin_at_epoch, begin_at_epoch + num_epochs):\n",
    "        # Run one epoch\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, begin_at_epoch + num_epochs))\n",
    "        # Compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (train_size + batch_size - 1) // batch_size\n",
    "        train_sess(sess, train_model_spec, num_steps, train_writer)\n",
    "\n",
    "        # Save weights\n",
    "        last_save_path = os.path.join(model_dir, 'last_weights', 'after-epoch')\n",
    "        last_saver.save(sess, last_save_path, global_step=epoch + 1)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (eval_size + batch_size - 1) // batch_size\n",
    "        metrics = evaluate_sess(sess, eval_model_spec, num_steps, eval_writer)\n",
    "\n",
    "        # If best_eval, best_save_path\n",
    "        eval_acc = metrics['accuracy']\n",
    "        if eval_acc >= best_eval_acc:\n",
    "            # Store new best accuracy\n",
    "            best_eval_acc = eval_acc\n",
    "            # Save weights\n",
    "            best_save_path = os.path.join(model_dir, 'best_weights', 'after-epoch')\n",
    "            best_save_path = best_saver.save(sess, best_save_path, global_step=epoch + 1)\n",
    "            logging.info(\"- Found new best accuracy, saving in {}\".format(best_save_path))\n",
    "            # Save best eval metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_eval_best_weights.json\")\n",
    "            save_dict_to_json(metrics, best_json_path)\n",
    "\n",
    "        # Save latest eval metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_eval_last_weights.json\")\n",
    "        save_dict_to_json(metrics, last_json_path)\n",
    "logging.info(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
